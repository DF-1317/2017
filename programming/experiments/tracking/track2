#!/usr/bin/python

""" Test the ability to identify and track an object.

  Our setup used 2 strips of 3M 8800? tape and a green led light ring.
  We ran a calibration program to see what the min/max HSV values to use for our range filter.
  This code will do a detection for the largest regions that remain from that filter, define
  a larger bounding box to contain those regions, and then track the video based on that bbox.
  A detection is repeated every X [100] number of frames to help out the tracking software stay
  on point.
  We use detection & tracking because tracking takes up so much less processing time/resources.
"""

import cv2
import numpy as np
import os
import sys
import json
import imutils
import socket
import time

DetectFrameRate       = 20   # How often to re-run the detection
SingleObjectThreshold = 0.25 # The size different needed to declare one region
ShowBbox              = True # Should we show the bbox image to the user?
UseErodeAndDilate     = False
# Network Stuff
IPADDR                = '10.13.17.102' # address of the RIO
PORTNUM               = 5800
netSock               = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, 0)

# Different color filters; color changes as we get closer/farther
Filters = (
    (np.array([0,50,120],np.uint8), np.array([100,255,255],np.uint8)),
    (np.array([0,10,220],np.uint8), np.array([100,255,255],np.uint8)),
    (np.array([0,10,100],np.uint8), np.array([100,255,220],np.uint8))
    )
FilterToUse = 0
FilterEntries = len(Filters)
FilterSwitchThreshold = float(0.90)
FilterRetries = 3

MinArea = 80

JumpCount = 0
JumpLimit = 6
JumpThreshold = 5

''' Detect our target

  We are given a mask that has already been filtered for the color range our
  target image falls into. Since we are using green lights, the range check is
  based on green.
  We do additional threshold checks and then find the contours.
  We need to see a few things [2] for our logic to work. If not, we return None as a bounding box.
'''
def detect(mask):
  ret, thresh = cv2.threshold(mask, 127,255,0)
  _, contours, _ = cv2.findContours( thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
  numContours = len(contours)
  print 'Number of contours ', numContours
  if (2 > numContours): return None
  
  areas = [cv2.contourArea(c) for c in contours]
  sorted = np.argsort(areas)
  print cv2.boundingRect(contours[sorted[numContours-1]])
  print cv2.boundingRect(contours[sorted[numContours-2]])
  print cv2.boundingRect(contours[sorted[numContours-3]])
  print cv2.boundingRect(contours[sorted[numContours-4]])
  
  maxIdxs = sorted[-2:][::-1]
  # Look for largest where the height is > width
  maxIdxs2 = []
  
  for areaLoop in reversed(range(numContours)):
      x,y,w,h = cv2.boundingRect(contours[sorted[areaLoop]])
      if (w > h): continue
      maxIdxs2.append(sorted[areaLoop])
      if (1 < len(maxIdxs2)): break
  maxIdxs = maxIdxs2
  if (2>len(maxIdxs)): return None
  
  x,y,w,h = cv2.boundingRect(contours[maxIdxs[0]])
  x2,y2,w2,h2 = cv2.boundingRect(contours[maxIdxs[1]])
  area1 = w*h
  area2 = w2*h2
  print "Areas are: ", area1, area2
  if (area1 < MinArea or area2 < MinArea): return None
  print "Coords 1: ", x, y, w, h
  print "Coords 2: ", x2, y2, w2, h2
  pct = 0
  if (area1>area2): pct = (area1-area2)/area1
  else: pct = (area2-area1)/area2
  # If close in size, adjust x,y,...; if large difference, use the coords of largest
  # Put in code for minimum area to consider; otherwise not a target
  # if ( SingleObjectThreshold > pct ):
  if (True):
      if (x < x2):
          w = x2+w2-x
      else:
          w = x+w-x2
          x = x2
      if (y < y2):
          h = y2+h2-y
      else:
          h = y+h-y2
          y = y2
          
  # The bounding box for our targets
  print "BBox: ", w, y, w, h
  return (x,y,w,h)
    
''' Track our target

   We initialize by grabbing a camera, and setting up our loop variable to trigger a
   detection on the very first time through the while(). We also setup our lower/upper
   filter boundaries used to filter the color range we want; the values from from playing
   with the calibration program.
'''
def track():
  global JumpCount
  global JumpThreshold
  global JumpLimit
  
  cap = None
  while (not cap):
      # cap = cv2.VideoCapture(0) # built in camera
      # cap = cv2.VideoCapture(1) # USB camera
      cap = cv2.VideoCapture('http://10.13.17.12/mjpg/video.mjpg') # IP camera
      ok, img = cap.read()  # try a read
      if (not ok): cap = None
      
  loops = DetectFrameRate
  tracker = None
  bbox = None
  lastArea = 1
  retryCount = 0
  FilterToUse = 0
  lower = Filters[FilterToUse][0]
  upper = Filters[FilterToUse][1]
  oldX = 0
  oldY = 0
  
  ''' Main tracking loop
  
    We grab a frame and change it to HSV to work with it in openCV.
    We check our loop to see if it is time to detect our target again; we are initialized
    to do that on the very first time through this code.
  '''
  while True:
    ok, img = cap.read()
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    loops += 1
    
    ''' Time to detect again
    
      We reset our loop and then create a mask for the colors of our target. The erosion and
      dilation help in erasing some of the noisy parts of the image which will help speed up
      the rest of the processing.
      We finish with creating a new tracker and feeding it the image and the new bounding box
      given us by detect().
    '''
    if (DetectFrameRate < loops):
      loops = 0
      area = 0
      print "Using filters: ", FilterToUse
      
      mask = cv2.inRange(hsv, lower, upper)
      if (UseErodeAndDilate):
	      mask = cv2.erode(mask,None,iterations=2)
	      mask = cv2.dilate(mask,None,iterations=2)
      
      # MIL, BOOSTING, KCF, TLD, MEDIANFLOW or GOTURN
      # MEDIAFLOW seems to work best for our job; all the others
      # had trouble keeping up, or just got lost
      tracker = cv2.Tracker_create("MEDIANFLOW")
      
      bbox = detect(mask)
      if None == bbox:
          bbox = (0,0,10,10)
      else:
          area = int(bbox[2]*bbox[3])
    
      # See if we had a big change in area; may need new filter
      print "Tracking areas: ", lastArea, area
      diff = float(area)/float(lastArea)
      if (FilterSwitchThreshold > diff):
        retryCount += 1
        if (retryCount > FilterRetries):
              print "Switching filters...", diff
              FilterToUse += 1
              if (FilterEntries<=FilterToUse): FilterToUse = 0
              lower = Filters[FilterToUse][0]
              upper = Filters[FilterToUse][1]
              mask = cv2.inRange(hsv, lower, upper)
              bbox = detect(mask)
              
      else: retryCount = 0
      
      # Or, if we have a big change in x/y we track it and change if always bouncing
      if (None == bbox or
          abs(bbox[0]-oldX) > JumpThreshold or
          abs(bbox[1]-oldY) > JumpThreshold): JumpCount +=2
      else: JumpCount -= 1
      if (JumpCount >= JumpLimit):
        JumpCount = 0
        FilterToUse += 1
        if (FilterEntries <= FilterToUse): FilterToUse = 0
        lower = Filters[FilterToUse][0]
        upper = Filters[FilterToUse][1]
        mask = cv2.inRange(hsv, lower, upper)
        bbox = detect(mask)
      if None == bbox:
          bbox = (0,0,10,10)
      
      ok = tracker.init(img, bbox)
      lastArea = area+1
      cv2.imshow('mask',mask)
    
    ''' React to tracking
    
      We send the parameters of the bounding box out onto the network in JSON format.
      We then check to see if we should superimpose a rectangle on the image..
    '''
    ok, bbox = tracker.update(img)
    x,y,w,h = bbox
    oldX = x
    oldY = y
    
    # A 'no find' is treated as a jump, each time!
    if (0 == x and x == y and 10 == w and w == h):
      w = 0
      h = 0
    pdata = {'x': x, 'y': y, 'w': w, 'h': h}
    netSock.sendto(json.dumps(pdata),(IPADDR,PORTNUM))
    
    if (ShowBbox):
      p1 = (int(x),int(y))
      p2 = (int(x + w), int(y + h))
      cv2.rectangle(img, p1, p2, (0,255,0), 3)
      cv2.imshow('img',img)
  
    ''' Wait forever
    
      We break out of tracking when the <esc> key is hit by the user, cleaning up as
      we leave.
    '''
    k = cv2.waitKey(30) & 0xff
    if 27 == k:
      break
  cap.release()
  cv2.destroyAllWindows()
  #msg = json.dumps(pdata)
  #netSock.sendto(msg,(IPADDR,PORTNUM))

if __name__ == '__main__' :
  track()

""" TODO: things to get tracking

  Like the calibrate, we want to capture each frame, periodically to a 'detect'/'find'
    operation and then track on that rectangle until the next detect time comes along.
    The calibrate code did the detect work every 50 frames; we can vary this, but for
    now that sounds good.
  * setup the RIO software to listen for those packets and then react to coordinates
  * the coordinates reported will be for the center of the bounding rectangle
  * we need to match the coordinate system of the image and use it.
  
  + Needed to: sudo apt-get install python-pip
  + pip install imutils
  
  * Need to read from a network camera, not a USB camera
"""
